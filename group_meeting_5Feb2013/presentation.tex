\documentclass{beamer}
\usetheme{Frankfurt}
\usepackage{tikz}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\title{Dimensionality Reduction for \\Developmental Biology}
%\subtitle[]{Estimation of numerical errors}
\author[C. Dsilva]{Carmeline Dsilva}
\institute[PU]{
  Department of Chemical and Biological Engineering\\
  Princeton University
  Princeton, NJ 08544\\[1ex]
  \texttt{cdsilva@princeton.edu}
}
\date[February 2013]{5 February 2013}

\begin{document}

\begin{frame}[plain]
  \titlepage
\end{frame}

\section{Motivation}

\begin{frame}{Motivation}
    \begin{itemize}
        \item Time series data is prevalent in many applications
        \item It can be difficult to determine the times in an experiment
        \item We would like an ``automated'' way to order data in time
    \end{itemize}

    \centering
    \includegraphics[width=0.4\textwidth]{data1_scrambled.jpg}
    \hspace{0.1in}
    \includegraphics[width=0.4\textwidth]{data1_unscrambled.jpg}

    Above are data from one of Bomyi's experiments.
    Each row is the concentration profile of a morphogen in a fly.
    On the left, the data are scrambled (since each fly is at a different time).
    On the right, the data are ordered using principal component analysis (PCA) to get an idea of the temporal evolution of the morphogen.

    \begin{tikzpicture}[overlay]
    \draw [->] (-0.5,5)--(0.5,5);
    \end{tikzpicture}

\end{frame}

\begin{frame}{Example}

    \begin{columns}[T]
        \begin{column}{0.3 \textwidth}
            \includegraphics[width=\textwidth]{example1_all.jpg}

            {\Tiny This data is clearly ordered in time \par}
        \end{column}

        \begin{column}{0.3 \textwidth}
            \includegraphics[width=\textwidth]{example1_coarse.jpg}

            {\Tiny We can look at a ``coarsened'' representation of the data with only three spatial components \par}
        \end{column}

        \begin{column}{0.3 \textwidth}
            \includegraphics[width=\textwidth]{example1_embedding.jpg}

            {\Tiny If we plot the three-dimensional data and color them in time, we see they fall on a line and are correlated along this line in time \par}
        \end{column}
    \end{columns}
    \vspace{0.1 in}

    \centering
   If we embedded the original data, we would see the same linear trend, but the data would be embedded in a very high-dimensional space and we could not visualize it.

   We would like an automatic way to find this line in higher dimensions, so we can then order our data appropriately.

\end{frame}

\section{Principal Component Analysis}

\begin{frame}{Principal Component Analysis (PCA)}
  \begin{itemize}
  \item The goal of PCA \footnote{\tiny Shlens, J. (2005) A Tutorial on Principal Component Analysis.} is to find the direction(s) of maximum variance in a data set
  \item In our case, we are assuming that there is one direction with significantly more variance than the others
  \item We assume that if we project our data along this line, we will get an embedding that is correlated with time
  \end{itemize}
\end{frame}

\begin{frame}{PCA: Covariance Matrix}
  \begin{itemize}
  \item We have $n$ {\bf data points}, each in $d$ dimensions, denoted $x_1, x_2, \dots, x_n \in \mathbb{R}^d$, where $x_{i,j}$ denotes the $j^{th}$ component of data point $i$
  \item We first define the {\bf mean-centered} data points $\hat{x}_i$ as $$\hat{x}_{i,j} = x_{i,j} - \frac{1}{n} \sum_{i=1}^{n} x_{i,j}$$
  \item Let $X \in \mathbb{R}^{n \times d}$ denote the data matrix, where the $i^{th}$ row of $X$ contains $\hat{x}_i$
  \item The {\bf covariance matrix} $R \in \mathbb{R}^{d \times d}$ can be estimated as $$R = \frac{1}{n-1} X^T X$$
  \end{itemize}
\end{frame}

\begin{frame}{PCA: Eigendecomposition}
\begin{columns}
\begin{column}{0.7\textwidth}
  \begin{itemize}
    \item We then compute the {\bf eigenvectors} $v_1, v_2, \dots, v_d$ and {\bf eigenvalues} $\lambda_1, \lambda_2, \dots, \lambda_d$ of $R$ (all eigenvectors are normalized to 1)
    \item By construction, $R$ is symmetric and positive semi-definite and is therefore guaranteed to have real non-negative eigenvalues and orthogonal real eigenvectors
    \item We {\bf order} the eigenvector/eigenvalue pairs such that $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_d$
    \item $v_1, v_2, \dots, v_d$ are called the {\bf principal components}, and $\lambda_j$ measures the {\bf variance} captured by principal component $j$
  \end{itemize}
  \end{column}

  \begin{column}{0.3\textwidth}
        \includegraphics[width=\textwidth]{PCA_spectrum1.jpg}\\
        {\tiny The PCA eigenvalue spectrum; each eigenvalue measures the energy captured by the corresponding eigenvector \par}

        \vspace{0.3 in}

        \includegraphics[width=\textwidth]{PCA_mode1.jpg}\\
        {\tiny The first principal component \par}

  \end{column}
  \end{columns}
\end{frame}

\begin{frame}{PCA: Ordering and Projections}
    \begin{itemize}
        \item We can calculate the projection of data point $i$ onto principal component $j$ as $a_{i,j} = \langle \hat{x}_i, v_j \rangle = \sum_{k=1}^{d} \hat{x}_{i,k} v_{j,k}$
        \item $a_{i,j}$ can be viewed as a measure of how much principal component $j$ is represented in data point $i$
        \item We can order the data by $a_{i,1}$
        \item Each principal component defines an important ``direction'' in our data; we can compare data sets by comparing their principal components
    \end{itemize}

    \centering
    \includegraphics[height=0.3\textheight]{data1_scrambled.jpg}
    \includegraphics[height=0.3\textheight]{coeff_scrambled.jpg}\\
    {\Tiny The scrambled data (left) and the projection coefficient for each data point onto the first principal component (right).\\
    We will sort the data by the values of the projection coefficients. \par}

\end{frame}

\begin{frame}{Results}
  \centering
    Below are three replicate data sets.\\
    You can see the scrambled data (left), unscrambled data using PCA (middle), and first PCA mode (right)

    \includegraphics[width=0.3\textheight]{data1_scrambled.jpg}
    \includegraphics[width=0.3\textheight]{data1_unscrambled.jpg}
    \includegraphics[width=0.3\textheight]{PCA_mode1.jpg}\\
    \includegraphics[width=0.3\textheight]{data2_scrambled.jpg}
    \includegraphics[width=0.3\textheight]{data2_unscrambled.jpg}
    \includegraphics[width=0.3\textheight]{PCA_mode2.jpg}\\
    \includegraphics[width=0.3\textheight]{data3_scrambled.jpg}
    \includegraphics[width=0.3\textheight]{data3_unscrambled.jpg}
    \includegraphics[width=0.3\textheight]{PCA_mode3.jpg}

\end{frame}

\section{Diffusion Maps}

\begin{frame}{Some Problems with PCA}
    \centering
    \includegraphics[width=0.5\textwidth]{data1_unscrambled.jpg}
    \includegraphics[width=0.5\textwidth]{data_unscrambled_dmaps.jpg}\\
    The ordering of the large central peaks is comparable.\\
    However, the outer peaks are grouped together in the right picture (we could view this as ``better'').

\end{frame}

\begin{frame}{Nonlinear Orderings}
    \centering
    \includegraphics[width=0.75\textwidth]{coeff_12.jpg}

    The first two projection coefficients for the data (each point corresponds to a data point/concentration profile). \\
    We only use the first coefficient in the PCA ordering.

\end{frame}

\begin{frame}{Nonlinear Orderings}
    \centering
    \includegraphics[width=0.75\textwidth]{coeff_12_colored.jpg}

    The ordering of the data produced with this coloring would perhaps be better. \\
    PCA cannot detect this ordering because it is nonlinear.

\end{frame}

\begin{frame}{Diffusion Maps (DMAPS)}
\centering
Unlike PCA, diffusion maps \footnote{ \tiny Coifman {\em et al}, PNAS, 2005.} is a {\em nonlinear} dimensionality reduction technique

    \includegraphics[width=0.5\textwidth]{example1_embedding.jpg}
    \includegraphics[width=0.5\textwidth]{dmaps_spiral.jpg}

On the left you see data that has been ordered and colored using PCA.
On the right you see data that has been ordered and colored using DMAPS.
PCA would not be able to order the data on the right ``correctly'', since the curve is one-dimensional, but non-linear.

\end{frame}

\begin{frame}{DMAPS: Compute Distances}
\begin{itemize}
    \item Again we have data $x_1, x_2, \dots, x_n \in \mathbb{R}^d$

    \item We need a {\bf distance metric} $d(x_i, x_j)$ for our data; it is typical to use the Euclidean distance, although other metrics can be used
    \item We first construct the matrix $W \in \mathbb{R}^{n \times n}$, with
    $$W_{ij} = \exp \left( -\frac{d^2 (x_i, x_j)}{\epsilon} \right) $$
    where $\epsilon$ is a characteristic distance

    \item We define the diagonal matrix $D$ with $D_{ii} = \sum_{j=1}^{n} W_{ij}$
\end{itemize}
\end{frame}

\begin{frame}{DMAPS: Eigendecomposition}
\begin{itemize}
    \item We compute the {\bf eigenvectors} $v_1, v_2, \dots, v_n$ and {\bf eigenvalues} $\lambda_1, \lambda_2, \dots, \lambda_n$ of $D^{-1} W$.

    \item We {\bf order} the eigenvector/eigenvalue pairs such that $|\lambda_1| \ge |\lambda_2| \ge \dots \ge |\lambda_n|$

    \item Since $D^{-1}W \sim D^{-1/2} \left( D^{-1/2} W D^{-1/2} \right) D^{1/2}$, and $D^{-1/2} W D^{-1/2}$ is symmetric, $D^{-1}W$ is guaranteed to have real eigenvalues and eigenvectors.

    \item $D^{-1}W$ is row-stochastic, and so $\lambda_1 = 1$ and $v_1 =  \vec{1}$

    \item $|\lambda_i|$ provides a measure of the ``importance'' of eigenvector $i$.

    \item Based on the eigenvalues $\lambda_i$, we choose to use $m < n$ eigenvectors and define a {\bf diffusion map} $x_i \mapsto \left( v_2(i), v_3(i), \dots, v_m(i) \right)$

    \item In the time-series case, we assume there is only one important mode; our map is $x_i \mapsto v_2(i)$, and we order our data by $v_2(i)$

\end{itemize}
\end{frame}

\begin{frame}{Results}
You can see the scrambled data (left), unscrambled data using PCA (middle), and unscrambled data using DMAPS (right)
    \centering
    \includegraphics[width=0.3\textheight]{data1_scrambled.jpg}
    \includegraphics[width=0.3\textheight]{data1_unscrambled.jpg}
    \includegraphics[width=0.3\textheight]{data1_unscrambled_dmaps.jpg}\\
    \includegraphics[width=0.3\textheight]{data2_scrambled.jpg}
    \includegraphics[width=0.3\textheight]{data2_unscrambled.jpg}
    \includegraphics[width=0.3\textheight]{data2_unscrambled_dmaps.jpg}\\
    \includegraphics[width=0.3\textheight]{data3_scrambled.jpg}
    \includegraphics[width=0.3\textheight]{data3_unscrambled.jpg}
    \includegraphics[width=0.3\textheight]{data3_unscrambled_dmaps.jpg}

%With the first data set, there is a significantly better grouping of the secondary peaks when using DMAPS.
%In the second and third data sets, there is no significant advantage of using DMAPS versus PCA.
\end{frame}

\section{Theory of Diffusion Maps}

\begin{frame}{The Idea Behind DMAPS}
\begin{itemize}
\item We have data that lies on a curve, or more generally, a manifold
\item We assume that the manifold is smooth enough so that locally, it is linear and we trust the Euclidean distances
\item We want a method to compute ``good'' distances between all pairs of data points
\end{itemize}
\end{frame}

\begin{frame}{A Random Walk on the Data}
\begin{itemize}
\item Given data points $x_i$ and $x_j$, what would be a good distance measure between them?
\item We can imagine
\begin{itemize}
\item starting a swarm of random walkers at $x_i$
\item starting a swarm of random walkers at $x_j$
\item letting the random walkers ``hop'' from data point to data point, but they can only hop to ``nearby'' data points
\item after some time, we compare where the random walkers starting from $x_i$ are, and where the random walkers starting from $x_j$ are
\item if $x_i$ and $x_j$ are ``close'', then the two groups of random walkers should be in similar places.
\end{itemize}
\item We call this distance the {\bf diffusion distance}
\item We can mathematically formalize such a thought experiment
\end{itemize}
\end{frame}

\begin{frame}{Random Walker: The Math}
\begin{itemize}
\item We first construct a {\bf graph with weighted edges} on which our random walkers can ``hop''
\begin{itemize}
\item Each node of our graph is a data point
\item Each pair of nodes is connected by an edge with weight $W_{ij} = \exp \left( -\frac{d^2(x_i, x_j)}{\epsilon} \right)$
\item High-weight edges have a high probability of being traversed by a random walker
\item There is a large probability to hop to points $< \epsilon$ away and a small probability to hop to points $> \epsilon$ away
\end{itemize}
\item We then place these edge weights into a matrix $W \in \mathbb{R}^{n \times n}$
\item We define the {\bf probability transition matrix} $A$ for a random walker on our graph by normalizing the rows of $W$ (i.e. $A = D^{-1}W$)
\item If $\pi$ is an initial distribution of random walkers on the data, then $\pi A$ is the distribution of random walkers after one timestep, and $\pi A^t$ is the distribution of random walkers after $t$ timesteps
\end{itemize}
\end{frame}

\begin{frame}{Random Walker: Calculating the Diffusion Distance}
\begin{itemize}
\item let $\pi_i$ be the vector with $\pi_i(i) = 1$ and $\pi_i(k \ne i) = 0$
\item So $\pi_i A^t$ is the {\bf probability distribution of random walkers} at time $t$ if we start a swarm of random walkers at $x_i$
\item However, $\pi_i A^t$ is just the $i^{th}$ row of $A^t$, denoted $A_{i,\cdot}^t$
\item To compare random walkers starting at $x_i$ and $x_j$, we need to {\bf compare rows} $i$ and $j$ of $A^t$
\item One can show that the diffusion distance is given by
$$ \| A_{i,\cdot}^t - A_{j,\cdot}^t \|^2 =\sum_{k=1}^{n} \left( \frac{A_{i,k} - A_{j,k}}{D_{kk}} \right)^2 =  \sum_{k=1}^{n} \lambda_k^{2t} \left( v_k(i) - v_k(j) \right)^2 $$
(we weight the norm by the degrees because higher degree nodes should ``accumulate'' more probability)
\item Therefore, embedding the data points using the eigenvectors $v_2, v_3, \dots, v_n$ {\bf preserves the diffusion distance }
\item The eigenvalues measure how much each eigenvector contributes to the diffusion distance
\end{itemize}
\end{frame}

\section{Conclusions}
\begin{frame}{Other applications of PCA and DMAPS}
\begin{itemize}
\item Although we have only been discussing one-dimensional data, both PCA and DMAPS are more generally applicable to cases when there are several important dimensions
\begin{itemize}
\item PCA can find lines, planes, and hyperplanes
\item DMAPS can find curves and general manifolds
\end{itemize}
\item Both methods use eigenvalues as a measure of importance for each component, and so the intrinsic dimensionality of the data set is selected by looking for a ``spectral gap'', i.e. an $m$ such that $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_m \gg \lambda_{m+1} \ge \dots \ge \lambda_n$
\end{itemize}
\end{frame}

\begin{frame}{Compare/Contrast/Summarize}
\begin{itemize}
\item PCA and DMAPS can find reduced representations of data
\item With time series data, PCA/DMAPS can find the one ``important'' dimension that is correlated with time
\item Both methods require a distance measure for the data
\item Both methods are based on eigendecomposition of a matrix
\begin{itemize}
\item PCA uses a $d \times d$ matrix, where $d$ is the dimension of the ambient space
\item DMAPS uses an $n \times n$ matrix, where $n$ is the number of data points; therefore, DMAPS becomes computationally prohibitive for large data sets
\end{itemize}
\item PCA provides principal components, which are the important direction(s) in the data; DMAPS provides no analog to principal components, only an analog to the projection coefficients $a_{i,j}$
\item DMAPS is nonlinear, and so can often uncover/capture more complex patterns in data than PCA can
\end{itemize}
\end{frame}

\end{document}
